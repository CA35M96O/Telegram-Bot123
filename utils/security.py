# utils/security.py
"""
å®‰å…¨æ¨¡å— - æä¾›æ¶æ„å†…å®¹æ£€æµ‹å’Œå¼‚å¸¸è¡Œä¸ºè¯†åˆ«åŠŸèƒ½

æœ¬æ¨¡å—æä¾›ä»¥ä¸‹å®‰å…¨åŠŸèƒ½ï¼š
- æ¶æ„æ¨¡å¼æ£€æµ‹ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
- æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
- ç”¨æˆ·è¡Œä¸ºåˆ†æ
- é™æµå’Œå°ç¦æœºåˆ¶
- å®‰å…¨äº‹ä»¶è®°å½•å’ŒæŠ¥å‘Š

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0
æœ€åæ›´æ–°: 2025-10-31
"""

import re
import time
import json
import hashlib
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict
from functools import wraps

from utils.cache import cache_manager
from utils.logging_utils import log_system_event

logger = logging.getLogger(__name__)

@dataclass
class SecurityEvent:
    """å®‰å…¨äº‹ä»¶æ•°æ®ç»“æ„"""
    user_id: int
    event_type: str
    severity: str  # LOW, MEDIUM, HIGH, CRITICAL
    details: str
    timestamp: float = 0
    
    def __post_init__(self):
        if self.timestamp == 0:
            self.timestamp = time.time()

class SecurityManager:
    """å®‰å…¨ç®¡ç†å™¨"""
    
    def __init__(self):
        # æ¶æ„æ¨¡å¼åˆ—è¡¨ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
        self.malicious_patterns = [
            # SQLæ³¨å…¥æ¨¡å¼
            r"(?i)(union|select|insert|update|delete|drop|create|alter|exec|execute).*",
            # XSSæ”»å‡»æ¨¡å¼
            r"(?i)(<script|javascript:|onload|onerror|onclick)",
            # è·¯å¾„éå†
            r"(\.\.\/|\.\/|\/\.\.|\.\.\\|\.\.\\\.\.)",
            # å‘½ä»¤æ³¨å…¥
            r"(?i)(system|exec|shell_exec|passthru|popen|proc_open)",
            # æ–‡ä»¶åŒ…å«
            r"(?i)(include|require)(_once)?\s*[\"'].*\.(php|jsp|asp|aspx)[\"']",
        ]
        
        # æ•æ„Ÿè¯åˆ—è¡¨
        self.sensitive_words = [
            "è‰²æƒ…", "èµŒåš", "æ¯’å“", "æš´åŠ›", "ææ€–ä¸»ä¹‰", "è¯ˆéª—",
            "porn", "gamble", "drugs", "violence", "terrorism", "fraud"
        ]
        
        # ç”¨æˆ·è¡Œä¸ºè®°å½•
        self.user_behavior = defaultdict(lambda: {
            'request_count': 0,
            'last_request': 0.0,
            'suspicious_actions': 0,
            'rate_limit_count': 0
        })
        
        # ç”¨æˆ·é»‘åå• {user_id: unban_time}
        self.user_blacklist = {}
        
        # é™æµå™¨ {user_id_action: RateLimiter}
        self.rate_limiters = {}
        
        # å®‰å…¨äº‹ä»¶è®°å½•
        self.security_events = []
        
        # åŠ è½½é¢„å®šä¹‰çš„æ¶æ„æ¨¡å¼
        self._load_malicious_patterns()
    
    def _load_malicious_patterns(self):
        """åŠ è½½é¢„å®šä¹‰çš„æ¶æ„æ¨¡å¼"""
        # å¯ä»¥ä»æ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½æ›´å¤šæ¨¡å¼
        logger.info(f"å·²åŠ è½½ {len(self.malicious_patterns)} ä¸ªæ¶æ„æ¨¡å¼å’Œ {len(self.sensitive_words)} ä¸ªæ•æ„Ÿè¯")
    
    def check_content_security(self, content: str) -> Tuple[bool, str]:
        """æ£€æŸ¥å†…å®¹å®‰å…¨æ€§
        
        Args:
            content: è¦æ£€æŸ¥çš„å†…å®¹
            
        Returns:
            (is_safe, reason): æ˜¯å¦å®‰å…¨åŠåŸå› 
        """
        if not content:
            return True, "empty_content"
        
        content_lower = content.lower()
        
        # æ£€æŸ¥æ¶æ„æ¨¡å¼
        for pattern in self.malicious_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return False, f"malicious_pattern_{pattern}"
        
        # æ£€æŸ¥æ•æ„Ÿè¯
        content_lower = content.lower()
        for word in self.sensitive_words:
            if word in content_lower:
                return False, f"sensitive_content_{word}"
        
        # é‡å¤å†…å®¹æ£€æµ‹
        content_hash = hashlib.md5(content.encode()).hexdigest()
        cache_key = f"content_hash_{content_hash}"
        
        if cache_manager.get_db_result(cache_key):
            return False, "duplicate_content"
        else:
            # ç¼“å­˜å†…å®¹å“ˆå¸Œï¼ˆ1å°æ—¶ï¼‰
            cache_manager.set_db_result(cache_key, True, 3600)
        
        return True, "safe_content"
    
    def check_rate_limit(self, user_id: int, action_type: str = 'global') -> Tuple[bool, str]:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¶…è¿‡é™æµ
        
        Args:
            user_id: ç”¨æˆ·ID
            action_type: æ“ä½œç±»å‹
            
        Returns:
            (allowed, reason): æ˜¯å¦å…è®¸åŠåŸå› 
        """
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨é»‘åå•ä¸­
        if self._is_user_blacklisted(user_id):
            return False, "user_blacklisted"
        
        # åˆ›å»ºé™æµå™¨é”®
        key = f"{user_id}_{action_type}"
        now = time.time()
        
        # è·å–æˆ–åˆ›å»ºé™æµå™¨
        if key not in self.rate_limiters:
            self.rate_limiters[key] = RateLimiter(max_requests=10, time_window=60)  # 10æ¬¡/åˆ†é’Ÿ
        
        limiter = self.rate_limiters[key]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™æµ
        if not limiter.allow_request():
            # è®°å½•é™æµäº‹ä»¶
            self._record_security_event(user_id, "RATE_LIMIT_EXCEEDED", "LOW", f"æ“ä½œç±»å‹: {action_type}")
            
            # æ›´æ–°ç”¨æˆ·è¡Œä¸º
            self.user_behavior[user_id]['rate_limit_count'] += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œæƒ©ç½š
            self._check_punishment(user_id, action_type)
            
            return False, "rate_limited"
        
        # æ›´æ–°ç”¨æˆ·è¡Œä¸º
        behavior = self.user_behavior[user_id]
        behavior['request_count'] += 1
        behavior['last_request'] = now
        
        return True, "allowed"
    
    def check_user_behavior(self, user_id: int, action_type: str) -> Tuple[bool, str]:
        """æ£€æŸ¥ç”¨æˆ·è¡Œä¸ºæ˜¯å¦å¼‚å¸¸
        
        Args:
            user_id: ç”¨æˆ·ID
            action_type: æ“ä½œç±»å‹
            
        Returns:
            (allowed, reason): æ˜¯å¦å…è®¸åŠåŸå› 
        """
        behavior = self.user_behavior[user_id]
        now = time.time()
        
        # æ£€æŸ¥çŸ­æ—¶é—´å†…è¯·æ±‚é¢‘ç‡
        if now - behavior['last_request'] < 0.1:  # 100mså†…è¿ç»­è¯·æ±‚
            behavior['suspicious_actions'] += 1
            
            # è®°å½•å¯ç–‘è¡Œä¸º
            if behavior['suspicious_actions'] >= 5:
                self._record_security_event(
                    user_id, "SUSPICIOUS_BEHAVIOR", "MEDIUM",
                    f"é«˜é¢‘è¯·æ±‚è¡Œä¸º, æ“ä½œç±»å‹: {action_type}"
                )
                return False, "suspicious_behavior"
        
        return True, "normal_behavior"
    
    def add_to_blacklist(self, user_id: int, duration: int = 3600, reason: str = ""):
        """æ·»åŠ ç”¨æˆ·åˆ°é»‘åå•"""
        unban_time = time.time() + duration
        self.user_blacklist[user_id] = unban_time
        
        self._record_security_event(
            user_id, "USER_BLACKLISTED", "HIGH",
            f"å°ç¦æ—¶é•¿: {duration}ç§’, åŸå› : {reason}"
        )
        
        log_system_event(
            "USER_BLACKLISTED",
            f"ç”¨æˆ· {user_id} è¢«å°ç¦ {duration} ç§’, åŸå› : {reason}",
            "WARNING"
        )
    
    def remove_from_blacklist(self, user_id: int):
        """ä»é»‘åå•ç§»é™¤ç”¨æˆ·"""
        if user_id in self.user_blacklist:
            del self.user_blacklist[user_id]
            
            self._record_security_event(
                user_id, "USER_UNBANNED", "MEDIUM",
                "æ‰‹åŠ¨è§£å°"
            )
    
    def get_security_report(self, hours: int = 24) -> Dict[str, Any]:
        """ç”Ÿæˆå®‰å…¨æŠ¥å‘Š"""
        cutoff_time = time.time() - (hours * 3600)
        
        recent_events = [
            event for event in self.security_events 
            if event.timestamp > cutoff_time
        ]
        
        # ç»Ÿè®¡äº‹ä»¶ç±»å‹
        event_counts = defaultdict(int)
        severity_counts = defaultdict(int)
        
        for event in recent_events:
            event_counts[event.event_type] += 1
            severity_counts[event.severity] += 1
        
        # è®¡ç®—é£é™©ç­‰çº§
        risk_score = (
            severity_counts['CRITICAL'] * 10 +
            severity_counts['HIGH'] * 5 +
            severity_counts['MEDIUM'] * 2 +
            severity_counts['LOW'] * 1
        )
        
        if risk_score > 50:
            risk_level = "ğŸ”´ é«˜é£é™©"
        elif risk_score > 20:
            risk_level = "ğŸŸ¡ ä¸­é£é™©"
        else:
            risk_level = "ğŸŸ¢ ä½é£é™©"
        
        return {
            'time_range': f"æœ€è¿‘ {hours} å°æ—¶",
            'total_events': len(recent_events),
            'event_types': dict(event_counts),
            'severity_distribution': dict(severity_counts),
            'risk_score': risk_score,
            'risk_level': risk_level,
            'active_blacklist': len(self.user_blacklist),
            'recent_events': recent_events[-10:]  # æœ€è¿‘10ä¸ªäº‹ä»¶
        }
    
    def cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸçš„é»‘åå•
        expired_users = [
            user_id for user_id, unban_time in self.user_blacklist.items()
            if unban_time <= now
        ]
        
        for user_id in expired_users:
            del self.user_blacklist[user_id]
            self._record_security_event(
                user_id, "AUTO_UNBANNED", "LOW",
                "è‡ªåŠ¨è§£å°"
            )
        
        # æ¸…ç†æ—§çš„é™æµå™¨
        expired_limiters = []
        for key, limiter in self.rate_limiters.items():
            if now - limiter.last_refill > 3600:  # 1å°æ—¶æœªä½¿ç”¨
                expired_limiters.append(key)
        
        for key in expired_limiters:
            del self.rate_limiters[key]
        
        logger.info(f"å®‰å…¨æ¸…ç†å®Œæˆ: è§£å° {len(expired_users)} ç”¨æˆ·, æ¸…ç† {len(expired_limiters)} ä¸ªé™æµå™¨")
    
    # ç§æœ‰æ–¹æ³•
    def _is_user_blacklisted(self, user_id: int) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨é»‘åå•ä¸­"""
        if user_id not in self.user_blacklist:
            return False
        
        if time.time() >= self.user_blacklist[user_id]:
            # è‡ªåŠ¨è§£å°
            del self.user_blacklist[user_id]
            return False
        
        return True
    
    def _update_user_behavior(self, user_id: int, action_type: str):
        """æ›´æ–°ç”¨æˆ·è¡Œä¸ºè®°å½•"""
        behavior = self.user_behavior[user_id]
        now = time.time()
        
        # é‡ç½®å¯ç–‘è¡Œä¸ºè®¡æ•°ï¼ˆå¦‚æœæ—¶é—´é—´éš”è¶³å¤Ÿï¼‰
        if now - behavior.get('last_request', 0) > 5:
            behavior['suspicious_actions'] = max(0, behavior['suspicious_actions'] - 1)
    
    def _check_punishment(self, user_id: int, action_type: str):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œæƒ©ç½š"""
        # è®¡ç®—ç”¨æˆ·åœ¨çŸ­æ—¶é—´å†…çš„é™æµæ¬¡æ•°
        recent_events = [
            event for event in self.security_events
            if (event.user_id == user_id and 
                event.event_type == "RATE_LIMIT_EXCEEDED" and
                time.time() - event.timestamp < 300)  # 5åˆ†é’Ÿå†…
        ]
        
        if len(recent_events) >= 5:  # 5åˆ†é’Ÿå†…5æ¬¡é™æµ
            # ä¸´æ—¶å°ç¦ç”¨æˆ·
            self.add_to_blacklist(user_id, 900, "é¢‘ç¹è§¦å‘é™æµ")  # 15åˆ†é’Ÿå°ç¦
    
    def _record_security_event(self, user_id: int, event_type: str, severity: str, details: str):
        """è®°å½•å®‰å…¨äº‹ä»¶"""
        event = SecurityEvent(
            user_id=user_id,
            event_type=event_type,
            severity=severity,
            details=details
        )
        
        self.security_events.append(event)
        
        # é«˜å±äº‹ä»¶ç«‹å³è®°å½•æ—¥å¿—
        if severity in ['HIGH', 'CRITICAL']:
            log_system_event(
                f"SECURITY_{event_type}",
                f"ç”¨æˆ· {user_id}: {details}",
                "WARNING" if severity == 'HIGH' else "ERROR"
            )

class RateLimiter:
    """é™æµå™¨"""
    
    def __init__(self, max_requests: int = 10, time_window: int = 60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []
        self.last_refill = time.time()
    
    def allow_request(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚"""
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸè¯·æ±‚è®°å½•
        self.requests = [req_time for req_time in self.requests if now - req_time < self.time_window]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.requests) >= self.max_requests:
            return False
        
        # è®°å½•å½“å‰è¯·æ±‚
        self.requests.append(now)
        self.last_refill = now
        return True

# å…¨å±€å®‰å…¨ç®¡ç†å™¨å®ä¾‹
security_manager = SecurityManager()

# å®‰å…¨è£…é¥°å™¨
def rate_limit(action_type: str = 'global'):
    """é™æµè£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(update, context):
            user = update.effective_user
            if not user:
                return
            
            # æ£€æŸ¥é™æµ
            allowed, reason = security_manager.check_rate_limit(user.id, action_type)
            
            if not allowed:
                if reason == "rate_limited":
                    if update.callback_query:
                        update.callback_query.answer("âš ï¸ æ“ä½œè¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•", show_alert=True)
                    else:
                        update.message.reply_text("âš ï¸ æ“ä½œè¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•")
                elif reason == "user_blacklisted":
                    if update.callback_query:
                        update.callback_query.answer("ğŸš« æ‚¨æš‚æ—¶æ— æ³•ä½¿ç”¨æ­¤åŠŸèƒ½", show_alert=True)
                    else:
                        update.message.reply_text("ğŸš« æ‚¨æš‚æ—¶æ— æ³•ä½¿ç”¨æ­¤åŠŸèƒ½")
                return
            
            # æ£€æŸ¥ç”¨æˆ·è¡Œä¸º
            behavior_ok, behavior_reason = security_manager.check_user_behavior(user.id, action_type)
            
            if not behavior_ok:
                if update.callback_query:
                    update.callback_query.answer("âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸ºï¼Œè¯·ç¨åå†è¯•", show_alert=True)
                else:
                    update.message.reply_text("âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸ºï¼Œè¯·ç¨åå†è¯•")
                return
            
            # æ‰§è¡ŒåŸå‡½æ•°
            return func(update, context)
        return wrapper
    return decorator

def content_security_check():
    """å†…å®¹å®‰å…¨æ£€æŸ¥è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(update, context):
            # æ£€æŸ¥æ¶ˆæ¯å†…å®¹å®‰å…¨æ€§
            if update.message and update.message.text:
                is_safe, reason = security_manager.check_content_security(update.message.text)
                
                if not is_safe:
                    user = update.effective_user
                    if user:
                        security_manager._record_security_event(
                            user.id, "CONTENT_BLOCKED", "MEDIUM",
                            f"å†…å®¹è¢«é˜»æ­¢: {reason}"
                        )
                    
                    if update.callback_query:
                        update.callback_query.answer("âš ï¸ å†…å®¹åŒ…å«ä¸å®‰å…¨å…ƒç´ ", show_alert=True)
                    else:
                        update.message.reply_text("âš ï¸ å†…å®¹åŒ…å«ä¸å®‰å…¨å…ƒç´ ")
                    return
            
            # æ‰§è¡ŒåŸå‡½æ•°
            return func(update, context)
        return wrapper
    return decorator