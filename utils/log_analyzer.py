# utils/log_analyzer.py
"""
æ™ºèƒ½æ—¥å¿—åˆ†æå™¨ - æ—¥å¿—æ¨¡å¼åˆ†æå’Œå¼‚å¸¸æ£€æµ‹

æœ¬æ¨¡å—æä¾›æ™ºèƒ½åŒ–çš„æ—¥å¿—åˆ†æåŠŸèƒ½ï¼š

ä¸»è¦åŠŸèƒ½ï¼š
- æ—¥å¿—æ¨¡å¼è¯†åˆ«ï¼šè‡ªåŠ¨è¯†åˆ«å¸¸è§çš„æ—¥å¿—æ¨¡å¼å’Œå¼‚å¸¸
- é”™è¯¯èšåˆï¼šç›¸ä¼¼é”™è¯¯çš„è‡ªåŠ¨åˆ†ç»„å’Œç»Ÿè®¡
- è¶‹åŠ¿åˆ†æï¼šæ—¥å¿—é‡ã€é”™è¯¯ç‡ç­‰è¶‹åŠ¿åˆ†æ
- å¼‚å¸¸æ£€æµ‹ï¼šåŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ—¥å¿—æ£€æµ‹
- æ—¥å¿—æœç´¢ï¼šé«˜æ•ˆçš„æ—¥å¿—æœç´¢å’Œè¿‡æ»¤
- æŠ¥å‘Šç”Ÿæˆï¼šè‡ªåŠ¨ç”Ÿæˆæ—¥å¿—åˆ†ææŠ¥å‘Š

æŠ€æœ¯ç‰¹æ€§ï¼š
- å®æ—¶æ—¥å¿—å¤„ç†ï¼šæ”¯æŒæµå¼æ—¥å¿—åˆ†æ
- å†…å­˜é«˜æ•ˆï¼šå¤§æ–‡ä»¶æ—¥å¿—çš„é«˜æ•ˆå¤„ç†
- æ¨¡å¼å­¦ä¹ ï¼šè‡ªåŠ¨å­¦ä¹ æ­£å¸¸æ—¥å¿—æ¨¡å¼
- å¯æ‰©å±•æ¡†æ¶ï¼šæ”¯æŒè‡ªå®šä¹‰åˆ†æè§„åˆ™

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 2.0
æœ€åæ›´æ–°: 2025-09-05
"""

import logging
import re
import os
import time
import json
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Pattern
from datetime import datetime, timedelta
from collections import defaultdict, Counter, deque
from dataclasses import dataclass, field
from enum import Enum

from utils.logging_utils import log_system_event
# æ—¶é—´å·¥å…·å‡½æ•°
from utils.time_utils import get_beijing_now

logger = logging.getLogger(__name__)

class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""
    ERROR_SPIKE = "error_spike"          # é”™è¯¯æ¿€å¢
    UNUSUAL_PATTERN = "unusual_pattern"   # å¼‚å¸¸æ¨¡å¼
    MISSING_LOGS = "missing_logs"        # æ—¥å¿—ç¼ºå¤±
    PERFORMANCE_ISSUE = "performance"     # æ€§èƒ½é—®é¢˜
    SECURITY_ISSUE = "security"          # å®‰å…¨é—®é¢˜

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: datetime
    level: LogLevel
    message: str
    module: str = ""
    user_id: Optional[int] = None
    tags: Dict[str, str] = field(default_factory=dict)
    
    @property
    def hash(self) -> str:
        """ç”Ÿæˆæ—¥å¿—æ¡ç›®çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºå»é‡ï¼‰"""
        content = f"{self.level.value}:{self.module}:{self.message}"
        return hashlib.md5(content.encode()).hexdigest()[:16]

@dataclass
class LogPattern:
    """æ—¥å¿—æ¨¡å¼"""
    pattern_id: str
    regex: Pattern
    description: str
    category: str
    severity: LogLevel
    count: int = 0
    first_seen: Optional[datetime] = None
    last_seen: Optional[datetime] = None
    
    def match(self, log_entry: LogEntry) -> bool:
        """æ£€æŸ¥æ—¥å¿—æ¡ç›®æ˜¯å¦åŒ¹é…æ­¤æ¨¡å¼"""
        return bool(self.regex.search(log_entry.message))

@dataclass
class LogAnomaly:
    """æ—¥å¿—å¼‚å¸¸"""
    anomaly_type: AnomalyType
    severity: LogLevel
    title: str
    description: str
    affected_logs: List[LogEntry]
    detected_at: datetime
    confidence: float  # 0.0 - 1.0
    
    @property
    def log_count(self) -> int:
        return len(self.affected_logs)

class LogParser:
    """æ—¥å¿—è§£æå™¨"""
    
    def __init__(self):
        self.patterns = self._compile_patterns()
    
    def _compile_patterns(self) -> Dict[str, Pattern]:
        """ç¼–è¯‘æ—¥å¿—è§£ææ¨¡å¼"""
        return {
            # æ ‡å‡†æ—¥å¿—æ ¼å¼
            'standard': re.compile(
                r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})\s+'
                r'(?P<level>\w+)\s+(?P<module>\S+)\s+-\s+(?P<message>.+)'
            ),
            # ç”¨æˆ·æ´»åŠ¨æ—¥å¿—
            'user_activity': re.compile(
                r'USER_ACTIVITY - ID:(?P<user_id>\d+) @(?P<username>\S+) - (?P<activity>.+)'
            ),
            # ç®¡ç†å‘˜æ“ä½œæ—¥å¿—
            'admin_operation': re.compile(
                r'ADMIN_OPERATION - ID:(?P<admin_id>\d+) @(?P<username>\S+) - (?P<operation>.+)'
            ),
            # ç³»ç»Ÿäº‹ä»¶æ—¥å¿—
            'system_event': re.compile(
                r'SYSTEM_EVENT - (?P<event_type>\w+) - (?P<details>.*)'
            ),
            # æŠ•ç¨¿äº‹ä»¶æ—¥å¿—
            'submission_event': re.compile(
                r'SUBMISSION_EVENT - ID:(?P<user_id>\d+) @(?P<username>\S+) - Submission:(?P<submission_id>\d+) - (?P<event_type>\w+)'
            ),
            # é”™è¯¯å †æ ˆ
            'traceback': re.compile(r'Traceback \(most recent call last\):')
        }
    
    def parse_log_line(self, line: str) -> Optional[LogEntry]:
        """è§£æå•è¡Œæ—¥å¿—"""
        line = line.strip()
        if not line:
            return None
        
        # å°è¯•æ ‡å‡†æ ¼å¼
        match = self.patterns['standard'].search(line)
        if match:
            try:
                timestamp = datetime.strptime(match.group('timestamp'), '%Y-%m-%d %H:%M:%S,%f')
                level = LogLevel(match.group('level'))
                message = match.group('message')
                module = match.group('module')
                
                # è§£æç‰¹æ®Šç±»å‹çš„æ—¥å¿—
                tags = self._extract_tags(message)
                user_id = self._extract_user_id(message)
                
                return LogEntry(
                    timestamp=timestamp,
                    level=level,
                    message=message,
                    module=module,
                    user_id=user_id,
                    tags=tags
                )
            except (ValueError, KeyError) as e:
                logger.warning(f"è§£ææ—¥å¿—å¤±è´¥: {e}, è¡Œ: {line}")
        
        return None
    
    def _extract_tags(self, message: str) -> Dict[str, str]:
        """ä»æ¶ˆæ¯ä¸­æå–æ ‡ç­¾"""
        tags = {}
        
        # æå–ç”¨æˆ·æ´»åŠ¨
        if 'USER_ACTIVITY' in message:
            match = self.patterns['user_activity'].search(message)
            if match:
                tags.update({
                    'type': 'user_activity',
                    'user_id': match.group('user_id'),
                    'username': match.group('username'),
                    'activity': match.group('activity')
                })
        
        # æå–ç®¡ç†å‘˜æ“ä½œ
        elif 'ADMIN_OPERATION' in message:
            match = self.patterns['admin_operation'].search(message)
            if match:
                tags.update({
                    'type': 'admin_operation',
                    'admin_id': match.group('admin_id'),
                    'username': match.group('username'),
                    'operation': match.group('operation')
                })
        
        # æå–ç³»ç»Ÿäº‹ä»¶
        elif 'SYSTEM_EVENT' in message:
            match = self.patterns['system_event'].search(message)
            if match:
                tags.update({
                    'type': 'system_event',
                    'event_type': match.group('event_type'),
                    'details': match.group('details')
                })
        
        return tags
    
    def _extract_user_id(self, message: str) -> Optional[int]:
        """ä»æ¶ˆæ¯ä¸­æå–ç”¨æˆ·ID"""
        patterns = [
            r'ID:(\d+)',
            r'user_id[=:](\d+)',
            r'ç”¨æˆ·[ï¼š:](\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, message)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue
        
        return None

class PatternDetector:
    """æ¨¡å¼æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.known_patterns: List[LogPattern] = []
        self.learned_patterns: Dict[str, LogPattern] = {}
        self._initialize_known_patterns()
    
    def _initialize_known_patterns(self):
        """åˆå§‹åŒ–å·²çŸ¥æ¨¡å¼"""
        patterns = [
            # é”™è¯¯æ¨¡å¼
            LogPattern(
                pattern_id="database_error",
                regex=re.compile(r'(database|sqlite|mysql|postgresql).*error', re.IGNORECASE),
                description="æ•°æ®åº“é”™è¯¯",
                category="database",
                severity=LogLevel.ERROR
            ),
            LogPattern(
                pattern_id="network_error",
                regex=re.compile(r'(network|connection|timeout|refused).*error', re.IGNORECASE),
                description="ç½‘ç»œè¿æ¥é”™è¯¯",
                category="network",
                severity=LogLevel.ERROR
            ),
            LogPattern(
                pattern_id="telegram_api_error",
                regex=re.compile(r'telegram.*api.*error', re.IGNORECASE),
                description="Telegram APIé”™è¯¯",
                category="telegram",
                severity=LogLevel.ERROR
            ),
            LogPattern(
                pattern_id="permission_error",
                regex=re.compile(r'permission.*denied|access.*denied|forbidden', re.IGNORECASE),
                description="æƒé™é”™è¯¯",
                category="security",
                severity=LogLevel.WARNING
            ),
            
            # æ­£å¸¸æ¨¡å¼
            LogPattern(
                pattern_id="user_submission",
                regex=re.compile(r'USER_ACTIVITY.*æŠ•ç¨¿'),
                description="ç”¨æˆ·æŠ•ç¨¿æ´»åŠ¨",
                category="user_activity",
                severity=LogLevel.INFO
            ),
            LogPattern(
                pattern_id="admin_review",
                regex=re.compile(r'ADMIN_OPERATION.*(å®¡æ ¸|é€šè¿‡|æ‹’ç»)'),
                description="ç®¡ç†å‘˜å®¡æ ¸æ“ä½œ",
                category="admin_activity",
                severity=LogLevel.INFO
            ),
            
            # ç³»ç»Ÿæ¨¡å¼
            LogPattern(
                pattern_id="system_startup",
                regex=re.compile(r'(startup|started|initialized)', re.IGNORECASE),
                description="ç³»ç»Ÿå¯åŠ¨",
                category="system",
                severity=LogLevel.INFO
            ),
            LogPattern(
                pattern_id="memory_warning",
                regex=re.compile(r'memory.*warning|å†…å­˜.*è­¦å‘Š', re.IGNORECASE),
                description="å†…å­˜è­¦å‘Š",
                category="performance",
                severity=LogLevel.WARNING
            )
        ]
        
        self.known_patterns = patterns
    
    def detect_patterns(self, log_entries: List[LogEntry]) -> Dict[str, LogPattern]:
        """æ£€æµ‹æ—¥å¿—æ¨¡å¼"""
        pattern_matches = {}
        
        for entry in log_entries:
            for pattern in self.known_patterns:
                if pattern.match(entry):
                    if pattern.pattern_id not in pattern_matches:
                        pattern_matches[pattern.pattern_id] = LogPattern(
                            pattern_id=pattern.pattern_id,
                            regex=pattern.regex,
                            description=pattern.description,
                            category=pattern.category,
                            severity=pattern.severity,
                            count=0,
                            first_seen=entry.timestamp,
                            last_seen=entry.timestamp
                        )
                    
                    pattern_matches[pattern.pattern_id].count += 1
                    pattern_matches[pattern.pattern_id].last_seen = entry.timestamp
        
        return pattern_matches

class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.baseline_stats = {}
        self.error_threshold = 0.1  # é”™è¯¯ç‡é˜ˆå€¼
        self.spike_threshold = 3.0  # æ¿€å¢é˜ˆå€¼ï¼ˆå€æ•°ï¼‰
    
    def detect_anomalies(self, log_entries: List[LogEntry], time_window: int = 3600) -> List[LogAnomaly]:
        """æ£€æµ‹æ—¥å¿—å¼‚å¸¸"""
        anomalies = []
        
        if not log_entries:
            return anomalies
        
        # é”™è¯¯æ¿€å¢æ£€æµ‹
        error_spike = self._detect_error_spike(log_entries, time_window)
        if error_spike:
            anomalies.append(error_spike)
        
        # æ—¥å¿—ç¼ºå¤±æ£€æµ‹
        missing_logs = self._detect_missing_logs(log_entries, time_window)
        if missing_logs:
            anomalies.append(missing_logs)
        
        # æ€§èƒ½é—®é¢˜æ£€æµ‹
        performance_issues = self._detect_performance_issues(log_entries)
        anomalies.extend(performance_issues)
        
        # å®‰å…¨é—®é¢˜æ£€æµ‹
        security_issues = self._detect_security_issues(log_entries)
        anomalies.extend(security_issues)
        
        return anomalies
    
    def _detect_error_spike(self, log_entries: List[LogEntry], time_window: int) -> Optional[LogAnomaly]:
        """æ£€æµ‹é”™è¯¯æ¿€å¢"""
        now = get_beijing_now()
        current_window_start = now - timedelta(seconds=time_window)
        
        # å½“å‰æ—¶é—´çª—å£çš„é”™è¯¯
        current_errors = [
            entry for entry in log_entries
            if entry.timestamp >= current_window_start and entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]
        ]
        
        # å‰ä¸€ä¸ªæ—¶é—´çª—å£çš„é”™è¯¯ï¼ˆç”¨ä½œåŸºçº¿ï¼‰
        previous_window_start = current_window_start - timedelta(seconds=time_window)
        previous_errors = [
            entry for entry in log_entries
            if previous_window_start <= entry.timestamp < current_window_start and entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]
        ]
        
        if not previous_errors and len(current_errors) > 5:
            # æ²¡æœ‰å†å²é”™è¯¯ä½†å½“å‰æœ‰å¤§é‡é”™è¯¯
            return LogAnomaly(
                anomaly_type=AnomalyType.ERROR_SPIKE,
                severity=LogLevel.WARNING,
                title="é”™è¯¯æ¿€å¢æ£€æµ‹",
                description=f"æ£€æµ‹åˆ°é”™è¯¯æ¿€å¢ï¼šå½“å‰{time_window//60}åˆ†é’Ÿå†…æœ‰{len(current_errors)}ä¸ªé”™è¯¯",
                affected_logs=current_errors,
                detected_at=now,
                confidence=0.8
            )
        
        if len(previous_errors) > 0:
            error_ratio = len(current_errors) / len(previous_errors)
            if error_ratio > self.spike_threshold:
                return LogAnomaly(
                    anomaly_type=AnomalyType.ERROR_SPIKE,
                    severity=LogLevel.WARNING,
                    title="é”™è¯¯æ¿€å¢æ£€æµ‹",
                    description=f"é”™è¯¯æ•°é‡æ¿€å¢{error_ratio:.1f}å€ï¼šä»{len(previous_errors)}å¢è‡³{len(current_errors)}",
                    affected_logs=current_errors,
                    detected_at=now,
                    confidence=min(0.9, error_ratio / 10)
                )
        
        return None
    
    def _detect_missing_logs(self, log_entries: List[LogEntry], time_window: int) -> Optional[LogAnomaly]:
        """æ£€æµ‹æ—¥å¿—ç¼ºå¤±"""
        if not log_entries:
            return None
        
        now = get_beijing_now()
        recent_logs = [
            entry for entry in log_entries
            if (now - entry.timestamp).total_seconds() <= time_window
        ]
        
        # å¦‚æœæœ€è¿‘æ—¶é—´çª—å£å†…æ—¥å¿—å¾ˆå°‘ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜
        if len(recent_logs) < 10 and time_window >= 3600:  # 1å°æ—¶å†…å°‘äº10æ¡æ—¥å¿—
            return LogAnomaly(
                anomaly_type=AnomalyType.MISSING_LOGS,
                severity=LogLevel.WARNING,
                title="æ—¥å¿—é‡å¼‚å¸¸åä½",
                description=f"è¿‡å»{time_window//60}åˆ†é’Ÿå†…ä»…æœ‰{len(recent_logs)}æ¡æ—¥å¿—",
                affected_logs=recent_logs,
                detected_at=now,
                confidence=0.6
            )
        
        return None
    
    def _detect_performance_issues(self, log_entries: List[LogEntry]) -> List[LogAnomaly]:
        """æ£€æµ‹æ€§èƒ½é—®é¢˜"""
        anomalies = []
        
        # æŸ¥æ‰¾æ€§èƒ½ç›¸å…³çš„æ—¥å¿—
        performance_keywords = ['slow', 'timeout', 'performance', 'è¶…æ—¶', 'ç¼“æ…¢', 'æ€§èƒ½']
        performance_logs = []
        
        for entry in log_entries:
            if any(keyword in entry.message.lower() for keyword in performance_keywords):
                performance_logs.append(entry)
        
        if len(performance_logs) > 5:  # å¦‚æœæœ‰å¤šä¸ªæ€§èƒ½ç›¸å…³æ—¥å¿—
            anomalies.append(LogAnomaly(
                anomaly_type=AnomalyType.PERFORMANCE_ISSUE,
                severity=LogLevel.WARNING,
                title="æ€§èƒ½é—®é¢˜æ£€æµ‹",
                description=f"æ£€æµ‹åˆ°{len(performance_logs)}æ¡æ€§èƒ½ç›¸å…³æ—¥å¿—",
                affected_logs=performance_logs,
                detected_at=get_beijing_now(),
                confidence=0.7
            ))
        
        return anomalies
    
    def _detect_security_issues(self, log_entries: List[LogEntry]) -> List[LogAnomaly]:
        """æ£€æµ‹å®‰å…¨é—®é¢˜"""
        anomalies = []
        
        # æŸ¥æ‰¾å®‰å…¨ç›¸å…³çš„æ—¥å¿—
        security_keywords = ['unauthorized', 'forbidden', 'attack', 'malicious', 'æœªæˆæƒ', 'æ¶æ„']
        security_logs = []
        
        for entry in log_entries:
            if any(keyword in entry.message.lower() for keyword in security_keywords):
                security_logs.append(entry)
        
        if security_logs:
            anomalies.append(LogAnomaly(
                anomaly_type=AnomalyType.SECURITY_ISSUE,
                severity=LogLevel.ERROR,
                title="å®‰å…¨é—®é¢˜æ£€æµ‹",
                description=f"æ£€æµ‹åˆ°{len(security_logs)}æ¡å®‰å…¨ç›¸å…³æ—¥å¿—",
                affected_logs=security_logs,
                detected_at=get_beijing_now(),
                confidence=0.9
            ))
        
        return anomalies

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, log_directory: str = "logs"):
        self.log_directory = log_directory
        self.parser = LogParser()
        self.pattern_detector = PatternDetector()
        self.anomaly_detector = AnomalyDetector()
        self.analysis_cache = {}
    
    def analyze_logs(self, hours: int = 24) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        cutoff_time = get_beijing_now() - timedelta(hours=hours)
        
        # è¯»å–æ—¥å¿—æ–‡ä»¶
        log_entries = self._read_log_files(cutoff_time)
        
        if not log_entries:
            return {
                'error': 'æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶æˆ–æ—¥å¿—ä¸ºç©º',
                'log_count': 0,
                'analysis_time': get_beijing_now().isoformat()
            }
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = self._generate_basic_stats(log_entries)
        
        # æ¨¡å¼æ£€æµ‹
        patterns = self.pattern_detector.detect_patterns(log_entries)
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies = self.anomaly_detector.detect_anomalies(log_entries)
        
        # è¶‹åŠ¿åˆ†æ
        trends = self._analyze_trends(log_entries)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'analysis_time': get_beijing_now().isoformat(),
            'time_range': f"æœ€è¿‘ {hours} å°æ—¶",
            'log_count': len(log_entries),
            'basic_stats': basic_stats,
            'patterns': {
                pattern_id: {
                    'description': pattern.description,
                    'category': pattern.category,
                    'count': pattern.count,
                    'severity': pattern.severity.value
                }
                for pattern_id, pattern in patterns.items()
            },
            'anomalies': [
                {
                    'type': anomaly.anomaly_type.value,
                    'severity': anomaly.severity.value,
                    'title': anomaly.title,
                    'description': anomaly.description,
                    'log_count': anomaly.log_count,
                    'confidence': anomaly.confidence,
                    'detected_at': anomaly.detected_at.isoformat()
                }
                for anomaly in anomalies
            ],
            'trends': trends
        }
        
        return report
    
    def _read_log_files(self, cutoff_time: datetime) -> List[LogEntry]:
        """è¯»å–æ—¥å¿—æ–‡ä»¶"""
        log_entries = []
        
        if not os.path.exists(self.log_directory):
            logger.warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_directory}")
            return log_entries
        
        log_files = [
            f for f in os.listdir(self.log_directory)
            if f.endswith('.log') and not f.startswith('.')
        ]
        
        for log_file in log_files:
            file_path = os.path.join(self.log_directory, log_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        entry = self.parser.parse_log_line(line)
                        if entry and entry.timestamp >= cutoff_time:
                            log_entries.append(entry)
            except Exception as e:
                logger.error(f"è¯»å–æ—¥å¿—æ–‡ä»¶ {log_file} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´æ’åº
        log_entries.sort(key=lambda x: x.timestamp)
        return log_entries
    
    def _generate_basic_stats(self, log_entries: List[LogEntry]) -> Dict[str, Any]:
        """ç”ŸæˆåŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        if not log_entries:
            return {}
        
        # æŒ‰çº§åˆ«ç»Ÿè®¡
        level_counts = Counter(entry.level for entry in log_entries)
        
        # æŒ‰æ¨¡å—ç»Ÿè®¡
        module_counts = Counter(entry.module for entry in log_entries)
        
        # æŒ‰å°æ—¶ç»Ÿè®¡
        hourly_counts = Counter(entry.timestamp.hour for entry in log_entries)
        
        # ç”¨æˆ·æ´»åŠ¨ç»Ÿè®¡
        user_activity = Counter(entry.user_id for entry in log_entries if entry.user_id)
        
        # é”™è¯¯ç‡è®¡ç®—
        total_logs = len(log_entries)
        error_logs = sum(count for level, count in level_counts.items() 
                        if level in [LogLevel.ERROR, LogLevel.CRITICAL])
        error_rate = error_logs / total_logs if total_logs > 0 else 0
        
        return {
            'total_logs': total_logs,
            'error_rate': error_rate,
            'by_level': {level.value: count for level, count in level_counts.items()},
            'by_module': dict(module_counts.most_common(10)),
            'by_hour': dict(hourly_counts),
            'unique_users': len(user_activity),
            'most_active_users': dict(user_activity.most_common(5)),
            'time_span': {
                'start': log_entries[0].timestamp.isoformat(),
                'end': log_entries[-1].timestamp.isoformat()
            }
        }
    
    def _analyze_trends(self, log_entries: List[LogEntry]) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—è¶‹åŠ¿"""
        if len(log_entries) < 2:
            return {}
        
        # æŒ‰å°æ—¶åˆ†ç»„
        hourly_data = defaultdict(lambda: {'total': 0, 'errors': 0})
        
        for entry in log_entries:
            hour_key = entry.timestamp.strftime('%Y-%m-%d %H:00')
            hourly_data[hour_key]['total'] += 1
            if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]:
                hourly_data[hour_key]['errors'] += 1
        
        # è®¡ç®—è¶‹åŠ¿
        hours = sorted(hourly_data.keys())
        if len(hours) < 2:
            return {}
        
        recent_hour = hourly_data[hours[-1]]
        previous_hour = hourly_data[hours[-2]]
        
        # è®¡ç®—å˜åŒ–
        log_volume_change = ((recent_hour['total'] - previous_hour['total']) / 
                           max(previous_hour['total'], 1)) * 100
        
        error_change = ((recent_hour['errors'] - previous_hour['errors']) / 
                       max(previous_hour['errors'], 1)) * 100
        
        return {
            'log_volume_change_percent': round(log_volume_change, 2),
            'error_change_percent': round(error_change, 2),
            'hourly_data': dict(hourly_data),
            'trend_summary': self._get_trend_summary(log_volume_change, error_change)
        }
    
    def _get_trend_summary(self, log_change: float, error_change: float) -> str:
        """è·å–è¶‹åŠ¿æ‘˜è¦"""
        if abs(log_change) < 10 and abs(error_change) < 10:
            return "ğŸ“Š æ—¥å¿—é‡å’Œé”™è¯¯ç‡ç›¸å¯¹ç¨³å®š"
        elif log_change > 20:
            if error_change > 20:
                return "âš ï¸ æ—¥å¿—é‡å’Œé”™è¯¯æ•°éƒ½æ˜¾è‘—å¢åŠ "
            else:
                return "ğŸ“ˆ æ—¥å¿—é‡æ˜¾è‘—å¢åŠ ï¼Œé”™è¯¯ç‡æ­£å¸¸"
        elif log_change < -20:
            return "ğŸ“‰ æ—¥å¿—é‡æ˜¾è‘—å‡å°‘"
        elif error_change > 50:
            return "ğŸš¨ é”™è¯¯ç‡æ˜¾è‘—å¢åŠ "
        elif error_change < -20:
            return "âœ… é”™è¯¯ç‡æ˜¾è‘—å‡å°‘"
        else:
            return "ğŸ“Š æ—¥å¿—è¶‹åŠ¿æ­£å¸¸"

# å…¨å±€æ—¥å¿—åˆ†æå™¨å®ä¾‹
log_analyzer = LogAnalyzer()

# ä¾¿æ·å‡½æ•°
def analyze_recent_logs(hours: int = 24) -> Dict[str, Any]:
    """åˆ†ææœ€è¿‘çš„æ—¥å¿—"""
    return log_analyzer.analyze_logs(hours)

def detect_log_anomalies(hours: int = 1) -> List[LogAnomaly]:
    """æ£€æµ‹æ—¥å¿—å¼‚å¸¸"""
    cutoff_time = get_beijing_now() - timedelta(hours=hours)
    log_entries = log_analyzer._read_log_files(cutoff_time)
    return log_analyzer.anomaly_detector.detect_anomalies(log_entries)

def get_log_patterns(hours: int = 24) -> Dict[str, LogPattern]:
    """è·å–æ—¥å¿—æ¨¡å¼"""
    cutoff_time = get_beijing_now() - timedelta(hours=hours)
    log_entries = log_analyzer._read_log_files(cutoff_time)
    return log_analyzer.pattern_detector.detect_patterns(log_entries)